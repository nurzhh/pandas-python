{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ данных популярных песен Spotify\n",
    "\n",
    "В этом ноутбуке мы проведем анализ данных о популярных песнях на Spotify. Мы загрузим данные, выполним предварительную обработку, визуализируем результаты и выполним отбор признаков.\n",
    "\n",
    "## Шаг 1: Импорт библиотек\n",
    "\n",
    "Импортируем необходимые библиотеки для анализа данных.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import chardet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 2: Загрузка данных\n",
    "\n",
    "Загрузим данные о популярных песнях на Spotify из CSV файла."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_encoding(file_path):\n",
    "    \"\"\"Определяет кодировку файла.\"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        result = chardet.detect(f.read())\n",
    "    return result['encoding']\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\"Загружает и возвращает DataFrame из CSV файла с определенной кодировкой.\"\"\"\n",
    "    encoding = detect_encoding(file_path)\n",
    "    return pd.read_csv(file_path, encoding=encoding)\n",
    "\n",
    "# Укажите путь к вашему файлу данных\n",
    "data_file = './data/Popular_Spotify_Songs.csv'\n",
    "df = load_data(data_file)\n",
    "\n",
    "# Проверим первые строки загруженного датафрейма\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 3: Предварительная обработка данных\n",
    "\n",
    "В этом шаге мы обработаем пропущенные значения и нормализуем числовые признаки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df):\n",
    "    \"\"\"Обрабатывает пропущенные значения в наборе данных.\"\"\"\n",
    "    df = df.copy()  # Создаем копию\n",
    "    df['key'] = df['key'].fillna('Unknown')\n",
    "    df['in_shazam_charts'] = df['in_shazam_charts'].fillna(0)\n",
    "    return df\n",
    "\n",
    "def normalize_data(df, numerical_columns):\n",
    "    \"\"\"Нормализует числовые признаки с использованием StandardScaler.\"\"\"\n",
    "    df[numerical_columns] = df[numerical_columns].apply(pd.to_numeric, errors='coerce')\n",
    "    scaler = StandardScaler()\n",
    "    df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "    return df\n",
    "\n",
    "def preprocess_data(file_path):\n",
    "    \"\"\"Выполняет полную предварительную обработку данных.\"\"\"\n",
    "    df = load_data(file_path)\n",
    "    df = handle_missing_values(df)\n",
    "    numerical_cols = ['streams', 'bpm', 'danceability_%', 'energy_%', \n",
    "                      'acousticness_%', 'instrumentalness_%', 'liveness_%', 'speechiness_%']\n",
    "    df = normalize_data(df, numerical_cols)\n",
    "    return df\n",
    "\n",
    "# Предварительная обработка данных\n",
    "df = preprocess_data(data_file)\n",
    "\n",
    "# Проверим форму датафрейма и наличие NaN значений\n",
    "print(\"Форма датафрейма:\", df.shape)\n",
    "print(\"Количество NaN после преобразования:\", df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 4: Визуализация данных\n",
    "\n",
    "Создадим визуализации, чтобы проанализировать данные.\n",
    "\n",
    "### 4.1 Тепловая карта корреляции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_matrix(df):\n",
    "    \"\"\"Строит тепловую карту корреляционной матрицы.\"\"\"\n",
    "    numeric_df = df.select_dtypes(include='number')\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm')\n",
    "    plt.title('Correlation Matrix')\n",
    "    plt.show()\n",
    "\n",
    "# Построим тепловую карту\n",
    "plot_correlation_matrix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Гистограмма распределения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(df, column_name):\n",
    "    \"\"\"Строит гистограмму для заданного числового признака.\"\"\"\n",
    "    df[column_name].hist(bins=30)\n",
    "    plt.title(f'Distribution of {column_name}')\n",
    "    plt.xlabel(column_name)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "# Гистограмма распределения для 'streams'\n",
    "plot_histogram(df, 'streams')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Диаграмма рассеяния"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter(df, x_col, y_col):\n",
    "    \"\"\"Строит scatter plot для двух признаков.\"\"\"\n",
    "    plt.scatter(df[x_col], df[y_col])\n",
    "    plt.title(f'Scatter Plot: {x_col} vs {y_col}')\n",
    "    plt.xlabel(x_col)\n",
    "    plt.ylabel(y_col)\n",
    "    plt.show()\n",
    "\n",
    "# Диаграмма рассеяния между 'streams' и 'bpm'\n",
    "plot_scatter(df, 'streams', 'bpm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шаг 5: Отбор признаков\n",
    "\n",
    "В этом шаге мы выполним отбор признаков с помощью RFE и PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def recursive_feature_elimination(X, y, n_features):\n",
    "    \"\"\"Применяет RFE для выбора наиболее важных признаков.\"\"\"\n",
    "    model = RandomForestClassifier()\n",
    "    rfe = RFE(model, n_features_to_select=n_features)\n",
    "    fit = rfe.fit(X, y)\n",
    "    return fit.support_\n",
    "\n",
    "def apply_pca(X, n_components=2):\n",
    "    \"\"\"Применяет метод главных компонент для сокращения размерности.\"\"\"\n",
    "    pca = PCA(n_components=n_components)\n",
    "    principal_components = pca.fit_transform(X)\n",
    "    return principal_components, pca.explained_variance_ratio_\n",
    "\n",
    "# Разделим признаки и целевой столбец\n",
    "X = df.drop(columns=['streams'])  # Замените 'streams' на нужный целевой столбец\n",
    "y = df['streams']  # Замените на нужный целевой столбец\n",
    "\n",
    "# Применим RFE для отбора признаков\n",
    "selected_features = recursive_feature_elimination(X, y, n_features=5)\n",
    "print(\"Выбранные признаки:\", X.columns[selected_features])\n",
    "\n",
    "# Применение PCA\n",
    "principal_components, variance_ratios = apply_pca(X)\n",
    "print(\"Principal components:\", principal_components)\n",
    "print(\"Explained variance ratio:\", variance_ratios)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
